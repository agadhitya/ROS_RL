#!/usr/bin/env python
#################################################################################
# Copyright 2018 ROBOTIS CO., LTD.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#################################################################################

# Authors: Gilbert #

import rospy
import os
import json
import numpy as np
import random
import time
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from collections import deque
from std_msgs.msg import Float32MultiArray
from src.turtlebot3_dqn.environment_stage_2 import Env

import torch
import torch.nn.functional as F
import torch.optim as optim
from PPO import PPO
from itertools import count
from torch.distributions import Categorical
import matplotlib.pyplot as plt


class Memory:
    def __init__(self):
        self.actions = []
        self.states = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []
        self.values = []
    
    def clear_memory(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.is_terminals[:]
        del self.values [:]
        

class ReinforceAgent():
    def __init__(self):
        self.pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
        self.dirPath = os.path.dirname(os.path.realpath(__file__))
        self.dirPath = self.dirPath.replace('turtlebot3_dqn/nodes', 'turtlebot3_dqn/save_model/stage_2_')
        self.result = Float32MultiArray()
        
        self.state_size = 28
        self.action_size = 5
        self.env = Env(self.action_size)
        self.load_model = False
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.log_interval = 20           # print avg reward in the interval
        self.max_episodes = 3000        # max training episodes
        self.max_timesteps = 6000         # max timesteps in one episode
        self.update_timestep = 100     # update policy every n timesteps
        self.save_model = 1000
        self.lr = 0.002
        self.betas = (0.9, 0.999)
        self.gamma = 0.99                # discount factor
        self.K_epochs = 4                # update policy for K epochs
        self.eps_clip = 0.2              # clip parameter for PPO
        self.memory = Memory()
        self.model = PPO(self.state_size,self.action_size).to(self.device)     
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=self.betas)
	self.episode_durations=[]
        
       

        if self.load_model:
            self.model.set_weights(load_model(self.dirPath + str(self.load_episode) + ".h5").get_weights())

            with open(self.dirPath + str(self.load_episode) + '.json') as outfile:
                param = json.load(outfile)
                self.epsilon = param.get('epsilon')

    def save_network(self, filename):
        torch.save(self.model.state_dict(),filename)

    def load_model(self, filename):
        self.model.load_state_dict(torch.load(filename,map_location='cpu'))

    def optimizeModel(self):
        print "Into Optimize"
        rewards = []
        discounted_reward = 0
        for reward, is_terminal in zip(reversed(self.memory.rewards), reversed(self.memory.is_terminals)):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + (self.gamma * discounted_reward)
            rewards.insert(0, discounted_reward)
        
        # Normalizing the rewards:
        rewards = torch.tensor(rewards).to(self.device)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)
        
        # convert list to tensor
        old_states = torch.stack(self.memory.states).to(self.device).detach()
        old_actions = torch.stack(self.memory.actions).to(self.device).detach()
	old_logprobs = torch.stack(self.memory.logprobs).to(self.device).detach()
        old_values = torch.stack(self.memory.values).to(self.device).detach()
        
        for k in range( 0,self.K_epochs):
            n_pi,n_v = self.model(old_states)
            n_log_prob = n_pi.log_prob(old_actions)           
           
            
            ratio = torch.exp(n_log_prob - old_logprobs)
            
            advantages = rewards - n_v.detach()
            loss_clipped_1 = ratio * advantages
            loss_clipped_2 = torch.clamp(ratio, 1-self.eps_clip, 1+self.eps_clip) * advantages
            loss_clipped = torch.min(loss_clipped_1, loss_clipped_2) 
            loss_clipped = loss_clipped.mean()
            
            #caclulate entropy for exploration
            pi_entropy_loss = n_pi.entropy()
            pi_entropy_loss = pi_entropy_loss.mean()
            
            clipped_value =old_values + (n_v - old_values).clamp(min=-self.eps_clip,max=self.eps_clip)
            vf_loss = torch.max((n_v - rewards) ** 2, (clipped_value - rewards) ** 2)
            vf_loss = 0.5 * vf_loss.mean()
            
            loss = -(loss_clipped - 0.5 * vf_loss + 0.01 * pi_entropy_loss)
                
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)
            self.optimizer.step()

    def plot_iterations(self):
	plt.figure(2)
	plt.clf()
	durations_t = torch.tensor(self.episode_durations,dtype=torch.float)
	plt.title("Average Reward vs Episode")
	plt.xlabel('Episode')
	plt.ylabel('Average Reward over 10 episodes')
	if len(durations_t)>=10:
		means = durations_t.unfold(0,10,1).mean(1).view(-1)
		means = torch.cat((torch.zeros(9),means))
		plt.plot(means.numpy())


    
    def train(self):
	pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
	pub_get_action = rospy.Publisher('get_action', Float32MultiArray, queue_size=5)

	result = Float32MultiArray
	get_action = Float32MultiArray
        score = 0
        avg_length = 0
        global_step = 0
	scores,episodes = [],[]
        for i_episode in range(0, self.max_episodes):
            state = self.env.reset()
	    score = 0
            for t in range(self.max_timesteps):
                
                # Running policy_old:
                state = torch.tensor(state,dtype=torch.float32,device=self.device)
                pi, v = self.model(state) 
                action = pi.sample()   
		
		self.memory.states.append(state)
                self.memory.actions.append(action)
                self.memory.values.append(v)
                self.memory.logprobs.append(pi.log_prob(action))
                
                state, reward, done = self.env.step(action.item())
                self.memory.rewards.append(reward)
                self.memory.is_terminals.append(done) 
		
		score += reward
                
		get_action.data=[action,score,reward]
		pub_get_action.publish(get_action)

		if t>1000:
		    rospy.loginfo("Time Out")
		    done = True		     		
				
		if i_episode%100==0:
		    self.save_network(self.dirPath+str(i_episode)+'_PPO.h5')	
		    plt.savefig(self.dirPath+str(i_episode)+'_PPO.png')

                if done:
		    result.data = [score,score]
		    pub_result.publish(result)
		    scores.append(score)
		    episodes.append(i_episode)

		    self.episode_durations.append(score)
	            self.plot_iterations()

		    rospy.loginfo('Ep: %d score: %.2f',i_episode, score)
                    break
		    
                global_step+=1
                # update if its time
                if global_step % self.update_timestep == 0:
                    print "Into optimize length after {} steps".format(global_step)
                    self.optimizeModel()
                    self.memory.clear_memory()
                    global_step = 0
            

            
            # logging
            '''if i_episode % self.log_interval == 0:
                avg_length = int(avg_length/self.log_interval)
                running_reward = ((running_reward/float(self.log_interval)))
                print 'Episode {} \t avg length: {} \t reward: {}'.format(i_episode, avg_length, running_reward)
                running_reward = 0
                avg_length = 0'''
                
               

if __name__ == '__main__':
    rospy.init_node('turtlebot3_dqn_stage_1')
    agent = ReinforceAgent()
    agent.train()
    
