#!/usr/bin/env python
#################################################################################
# Copyright 2018 ROBOTIS CO., LTD.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#################################################################################

# Authors: Gilbert #

import rospy
import os
import json
import numpy as np
import random
import time
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from collections import deque
from std_msgs.msg import Float32MultiArray
from src.turtlebot3_dqn.environment_stage_2 import Env

import torch
import torch.nn.functional as F
import torch.optim as optim
from PPO import PPO
from itertools import count
from torch.distributions import Categorical


class Memory:
    def __init__(self):
        self.actions = []
        self.states = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []
        self.values = []
    
    def clear_memory(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.is_terminals[:]
        del self.values [:]
        

class ReinforceAgent():
    def __init__(self):
        self.pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
        self.dirPath = os.path.dirname(os.path.realpath(__file__))
        self.dirPath = self.dirPath.replace('turtlebot3_dqn/nodes', 'turtlebot3_dqn/save_model/stage_2_')
        self.result = Float32MultiArray()
        
        self.state_size = 28
        self.action_size = 5
        self.env = Env(action_size)
        self.load_model = False
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.log_interval = 20           # print avg reward in the interval
        self.max_episodes = 3000        # max training episodes
        self.max_timesteps = 6000         # max timesteps in one episode
        self.update_timestep = 2000     # update policy every n timesteps
        self.save_model = 1000
        self.lr = 0.002
        self.betas = (0.9, 0.999)
        self.gamma = 0.99                # discount factor
        self.K_epochs = 4                # update policy for K epochs
        self.eps_clip = 0.2              # clip parameter for PPO
        self.memory = Memory()
        self.model = PPO(state_size,self.action_dim).to(self.device)     
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=self.betas)
        
       

        if self.load_model:
            self.model.set_weights(load_model(self.dirPath + str(self.load_episode) + ".h5").get_weights())

            with open(self.dirPath + str(self.load_episode) + '.json') as outfile:
                param = json.load(outfile)
                self.epsilon = param.get('epsilon')

    def save_network(self, filename):
        torch.save(self.policy_net.state_dict(),filename)

    def load_model(self, filename):
        self.policy_net.load_state_dict(torch.load(filename,map_location='cpu'))

    def optimizeModel(self):
        print ("Into Optimize")
        rewards = []
        discounted_reward = 0
        for reward, is_terminal in zip(reversed(self.memory.rewards), reversed(self.memory.is_terminals)):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + (self.gamma * discounted_reward)
            rewards.insert(0, discounted_reward)
        
        # Normalizing the rewards:
        rewards = torch.tensor(rewards).to(self.device)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)
        
        # convert list to tensor
        old_states = torch.cat(self.memory.states).to(self.device).detach()
        old_actions = torch.cat(self.memory.actions).to(self.device).detach()
        old_logprobs = torch.cat(self.memory.logprobs).to(self.device).detach()
        old_values = torch.cat(self.memory.values).to(self.device).detach()
        
        for k in range( 0,self.K_epochs):
            n_pi,n_v = self.model(old_states)
            n_log_prob = n_pi.log_prob(old_actions)           
           
            
            ratio = torch.exp(n_log_prob - old_logprobs)
            
            advantages = rewards - n_v.detach()
            loss_clipped_1 = ratio * advantages
            loss_clipped_2 = torch.clamp(ratio, 1-self.eps_clip, 1+self.eps_clip) * advantages
            loss_clipped = torch.min(loss_clipped_1, loss_clipped_2) 
            loss_clipped = loss_clipped.mean()
            
            #caclulate entropy for exploration
            pi_entropy_loss = n_pi.entropy()
            pi_entropy_loss = pi_entropy_loss.mean()
            
            clipped_value =old_values + (n_v - old_values).clamp(min=-self.eps_clip,max=self.eps_clip)
            vf_loss = torch.max((n_v - rewards) ** 2, (clipped_value - rewards) ** 2)
            vf_loss = 0.5 * vf_loss.mean()
            
            loss: torch.Tensor = -(loss_clipped - 0.5 * vf_loss + 0.01 * pi_entropy_loss)
                
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)
            self.optimizer.step()
        
    def train(self):
        running_reward = 0
        avg_length = 0
        timestep = 0
        for i_episode in range(0, self.max_episodes):
            state = self.env.reset()
            for t in range(self.max_timesteps):
                timestep += 1
            
                # Running policy_old:
                state = torch.tensor(state,dtype=torch.float32,device=self.device)
                pi, v = self.model(state) 
                action = pi.sample()     
                
                self.memory.states.append(state)
                self.memory.actions.append(action)
                self.memory.values.append(v)
                self.memory.logprobs.append(pi.log_prob(action))
                
                state, reward, done,info = self.env.step(action.item())
                self.memory.rewards.append(reward)
                self.memory.is_terminals.append(done) 
               
               # update if its time
                if timestep % self.update_timestep == 0:
                    print ("Into optimize length after {} steps".format(timestep)
                    self.optimizeModel()
                    self.memory.clear_memory()
                    timestep = 0
            
                running_reward += reward
                if done:
                    break
                
            avg_length += t
        
            # save model
            if i_episode > (self.save_model):
                print("########## Model Saved! ##########")
                save_network('./PPO_{}.pth'.format(i_episode))
                break
            
            # logging
            if i_episode % self.log_interval == 0:
                avg_length = int(avg_length/self.log_interval)
                running_reward = ((running_reward/float(self.log_interval)))
                print('Episode {} \t avg length: {} \t reward: {}'.format(i_episode, avg_length, running_reward))
                running_reward = 0
                avg_length = 0
                
               

if __name__ == '__main__':
    rospy.init_node('turtlebot3_dqn_stage_1')
    pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
    pub_get_action = rospy.Publisher('get_action', Float32MultiArray, queue_size=5)
    agent = ReinforceAgent()
    agent.train()
    